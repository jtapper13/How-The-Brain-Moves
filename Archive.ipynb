{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUDZwXEem+/n6SBjYbhgZO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook Overview\n","This notebook serves as an archive for some of the initial testing and experimentation done on the dataset that is not described but very much had an impact on the methods in the written report. This code is messy and is **not** expected to work; it is provided as a proof of concept what remains from the initial investigation."],"metadata":{"id":"vZqcSHpLcFq1"}},{"cell_type":"markdown","source":["Below is an example of an initial model architecture. Note that this was before the data was cleaned/when the baseline class was being separated from left and right."],"metadata":{"id":"Nb7f5NokduyS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jF7h0haJcDnd"},"outputs":[],"source":["from tensorflow.keras.layers import Reshape, LSTM\n","\n","def build_cnn_lstm(input_shape, num_classes=10):\n","  model = Sequential()\n","\n","  model.add(Conv2D(16, (5, 3), activation='relu', padding='same', input_shape=input_shape))\n","  model.add(Conv2D(16, (5, 3), activation='relu', padding='same', input_shape=input_shape))\n","  model.add(MaxPooling2D((2, 2)))\n","  model.add(Dropout(0.1))\n","\n","  model.add(Conv2D(32, (7, 3), activation='relu', padding='same'))\n","  model.add(Conv2D(32, (7, 3), activation='relu', padding='same'))\n","  model.add(MaxPooling2D((2, 2)))\n","  model.add(Dropout(0.1))\n","\n","  # Flattening CNN output\n","  model.add(Flatten())\n","\n","  # Reshaping for LSTM (time steps, features)\n","  model.add(Reshape((-1, 32)))  # Adjust the second dimension based on your CNN output size\n","\n","  # LSTM Layers (Temporal Sequence Learning)\n","  model.add(LSTM(32, return_sequences=True))\n","  model.add(LSTM(32))\n","  model.add(Dropout(0.2))\n","\n","  # Dense Layers for Classification\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dropout(0.2))\n","  model.add(Dense(9, activation='softmax'))  # Assuming 10 classes for classification\n","\n","  return model\n","\n","top_channels_model = build_cnn_lstm((101, 4, top_k), num_classes=10)\n","top_channels_model.summary()"]},{"cell_type":"markdown","source":["Belwo are notes during a period where I was experimenting with different kernel sizes: These are the top accuracies achieved at different combinations. Note that this was also before some vital accuracy increasing steps, such as data cleaning, final decisions on classes, etc. Some results are slightly confusing, as kernel sizes expand far beyond the time dimension, but it does suggest that kernel much larger than those used can be considered in a more complete grid search.\n","\n","\n","*In general validation loss is staying quite high*\n","\n","*full 11 x 11 kernels: 41.5%*\n","\n","*11 x 11 -> 15 x 15: 54.6%*\n","\n","*13 x 13 -> 19 x 19: 40%*\n","\n","*13, 15 -> 19, 23: 33.7%*\n","\n","*11-> 31: 59.4%*\n","\n","*11 x 3 -> 31 x 5: 58.7%*\n","\n","*11 x 5 -> 31 x 7: 65.1%*\n","\n","*11 x 5 -> 51 x 7: 65.4%*\n","\n","*11 x 5 -> 51 x 11: 66.3%*\n","\n","*11 x 5 -> 51 x 9: 55.9%*\n","\n","*31 x 5 -> 51 x 11: 60.3%*\n","\n","*11 x 5, 31 x 9, 51 x 15: 50.8%*\n","\n","*11 x 5, 31 x 9, 51 x 11: 55. 2%*\n","\n","*41 x 5, 51 x 7: 46%*\n","\n","*3 x 3 -> 11 x 5 -> 51 x 11: 55.6%*\n","\n","*5 x 5 -> 11 x 5 -> 51 x 11: 61.6%*\n","\n","*1 x 1 -> 11 x 5 -> 51 x 11: 64.8%*\n","\n","*11 x 3, 3 x 2 -> 64.8%*"],"metadata":{"id":"e5k9u0ZheO_N"}},{"cell_type":"markdown","source":["Below is a sample from initial training on PCA. In this case, training actually did not occur at all, leading to an initial hypothesis that the encoding method was by far the superior. Once our methods were finalized this proved to not be the case."],"metadata":{"id":"KGPxGtDxgQkd"}},{"cell_type":"code","source":["# Training autoencoder + CNN\n","\n","optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n","batch_size = 128\n","epochs = 100\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,\n","                                                  restore_best_weights=True)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95,\n","                              patience=5, min_lr=0.00001)\n","\n","pca_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","hist = pca_model.fit(X_pca_train, y_pca_train, callbacks=[early_stopping, reduce_lr],\n","                 epochs=epochs, batch_size=batch_size, validation_split=0.2)\n"],"metadata":{"id":"lxGlpx0QeS_X","executionInfo":{"status":"ok","timestamp":1750173569042,"user_tz":-120,"elapsed":26,"user":{"displayName":"Joshua Tapper","userId":"12596847093048438380"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Epoch 1/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 11s 718ms/step - accuracy: 0.0955 - loss: 1068.4323 - val_accuracy: 0.0794 - val_loss: 57.5287 - learning_rate: 0.0010\n","Epoch 2/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 0.1117 - loss: 29.4635 - val_accuracy: 0.1270 - val_loss: 2.2998 - learning_rate: 0.0010\n","Epoch 3/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 34ms/step - accuracy: 0.0868 - loss: 2.4491 - val_accuracy: 0.0992 - val_loss: 2.3026 - learning_rate: 0.0010\n","Epoch 4/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.0872 - loss: 2.3063 - val_accuracy: 0.0992 - val_loss: 2.3026 - learning_rate: 0.0010\n","Epoch 5/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1046 - loss: 2.3258 - val_accuracy: 0.0992 - val_loss: 2.3026 - learning_rate: 0.0010\n","Epoch 6/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.0944 - loss: 2.3026 - val_accuracy: 0.0992 - val_loss: 2.3026 - learning_rate: 0.0010\n","Epoch 7/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1052 - loss: 2.3024 - val_accuracy: 0.0992 - val_loss: 2.3026 - learning_rate: 0.0010\n","Epoch 8/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.0956 - loss: 2.3024 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.5000e-04\n","Epoch 9/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1070 - loss: 2.3672 - val_accuracy: 0.1032 - val_loss: 2.3025 - learning_rate: 9.5000e-04\n","Epoch 10/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1107 - loss: 2.4188 - val_accuracy: 0.1032 - val_loss: 2.3025 - learning_rate: 9.5000e-04\n","Epoch 11/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.0963 - loss: 2.3027 - val_accuracy: 0.1032 - val_loss: 2.3025 - learning_rate: 9.5000e-04\n","Epoch 12/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.0977 - loss: 2.3025 - val_accuracy: 0.1032 - val_loss: 2.3025 - learning_rate: 9.5000e-04\n","Epoch 13/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.1039 - loss: 2.3110 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.0250e-04\n","Epoch 14/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1091 - loss: 2.3022 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.0250e-04\n","Epoch 15/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.1026 - loss: 2.3122 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.0250e-04\n","Epoch 16/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.0997 - loss: 2.3033 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.0250e-04\n","Epoch 17/100\n","8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 0.1186 - loss: 2.3020 - val_accuracy: 0.1032 - val_loss: 2.3026 - learning_rate: 9.0250e-04"],"metadata":{"id":"ZGlSta9QgMC7"}},{"cell_type":"markdown","source":["Below is the code that was used to intitially test a full autoencoder trained without any link to a classifier. This model (and the classifier then tried on the encoding data) performed much worse than the singular model."],"metadata":{"id":"7G83q8_IhjSU"}},{"cell_type":"code","source":["epoched_total = epoched_total.transpose(2, 3, 1, 0)\n","epoched_total = epoched_total.reshape(-1, 508, 256)\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","input_shape = (51, 5, 256)\n","\n","inputs = layers.Input(shape=input_shape)\n","\n","# --- ENCODER ---\n","x = layers.Conv2D(128, (5, 1), activation='relu', padding='same', name='encoded_1')(inputs)\n","x = layers.Conv2D(64, (5, 1), activation='relu', padding='same')(x)\n","bottleneck = layers.Conv2D(32, (3, 1), activation='relu', padding='same', name='bottle')(x)\n","\n","# --- DECODER ---\n","x = layers.Conv2D(64, (5, 1), activation='relu', padding='same')(bottleneck)\n","x = layers.Conv2D(128, (5, 1), activation='relu', padding='same')(x)\n","decoded = layers.Conv2D(256, (5, 1), activation='linear', padding='same')(x)\n","# Model\n","autoencoder = models.Model(inputs, decoded)\n","autoencoder.compile(optimizer='adam', loss='mse')\n","autoencoder.summary()\n","\n","\n","X_auto = specs.copy()\n","print(X_auto.shape)\n","#X_auto = (X_auto - np.mean(X_auto)) / np.std(X_auto)\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,\n","                                                  restore_best_weights=True)\n","\n","autoencoder.fit(X_auto, X_auto,  # X is both input and target\n","                epochs=100,\n","                batch_size=16,\n","                validation_split=0.1,\n","                callbacks=[early_stopping],\n","                shuffle=True)\n","\n","# Extract encoder model\n","# Assume autoencoder is your trained model\n","encoder = models.Model(autoencoder.input, autoencoder.get_layer('bottle').output)\n","\n","# Apply the encoder to your data (e.g., X_train)\n","decoded_data = encoder.predict(specs)\n","#decoded_data = decoded_data.transpose(0, 2, 1)\n","\n","decoded_data.shape"],"metadata":{"id":"Hqfc0Iuah4sA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below are early examples of testing model architectures."],"metadata":{"id":"LO42EJIxid7t"}},{"cell_type":"code","source":["# 2D CNN for training on spectrograms\n","input_shape = (51, 5, 32)\n","output_shape = 5\n","\n","cnn = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Conv2D(512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.Conv2D(512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Dropout(0.1),\n","        layers.Conv2D(1024, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.Conv2D(1024, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n","        layers.Dropout(0.1),\n","        layers.Conv2D(2048, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.Conv2D(2048, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n","        layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n","        layers.Dropout(0.1),\n","        layers.Flatten(),\n","        layers.Dense(1024, activation=\"relu\"),\n","        layers.Dropout(0.1),\n","        layers.Dense(output_shape, activation=\"softmax\"),\n","    ]\n",")\n","\n","cnn.summary()\n","\n","# LSTM\n","input_shape = (13, 33, 32)  # (time, freq, channels)\n","output_shape = 5\n","\n","lstm = keras.Sequential([\n","    # Input shape: (51 time steps, 5 freq bins, 256 channels)\n","    layers.Input(shape=input_shape),\n","\n","    # 1. Frequency-domain processing (applied per time step)\n","    layers.TimeDistributed(layers.Conv1D(32, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.Conv1D(32, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.MaxPooling1D(2)),\n","    #layers.TimeDistributed(layers.Dropout(0.1)),\n","\n","    # 2. Deeper frequency processing\n","    layers.TimeDistributed(layers.Conv1D(64, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.Conv1D(64, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.MaxPooling1D(2)),\n","    layers.TimeDistributed(layers.Dropout(0.1)),\n","\n","    layers.TimeDistributed(layers.Conv1D(128, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.Conv1D(128, 3, activation='relu', padding='same')),\n","    layers.TimeDistributed(layers.MaxPooling1D(2, padding='same')),\n","    layers.TimeDistributed(layers.Dropout(0.15)),\n","\n","    # 3. Prepare for LSTM - output will be (None, 51, remaining_features)\n","    layers.TimeDistributed(layers.Flatten()),\n","\n","    # 4. Temporal processing\n","    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n","    layers.Bidirectional(layers.LSTM(128)),\n","    layers.Dropout(0.4),\n","\n","    # 5. Classifier\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.4),\n","    layers.Dense(output_shape, activation='softmax')\n","])\n","\n","lstm.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"_if1h3flisYX","executionInfo":{"status":"ok","timestamp":1750174741348,"user_tz":-120,"elapsed":10,"user":{"displayName":"Joshua Tapper","userId":"12596847093048438380"}},"outputId":"89882ec1-aab9-472c-b3ab-0d5017f5cb0b"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# 2D CNN for training on spectrograms\\ninput_shape = (51, 5, 32)\\noutput_shape = 5\\n\\ncnn = keras.Sequential(\\n    [\\n        keras.Input(shape=input_shape),\\n        layers.Conv2D(512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.Conv2D(512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Dropout(0.1),\\n        layers.Conv2D(1024, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.Conv2D(1024, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.MaxPooling2D(pool_size=(2, 2), padding=\\'same\\'),\\n        layers.Dropout(0.1),\\n        layers.Conv2D(2048, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.Conv2D(2048, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\\n        layers.MaxPooling2D(pool_size=(2, 2), padding=\\'same\\'),\\n        layers.Dropout(0.1),\\n        layers.Flatten(),\\n        layers.Dense(1024, activation=\"relu\"),\\n        layers.Dropout(0.1),\\n        layers.Dense(output_shape, activation=\"softmax\"),\\n    ]\\n)\\n\\ncnn.summary()'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Below is some leftover code from when we were trying to create our spectrograms dynamically within our full model (because the initial idea was to perform encoding on the raw data). Eventually this was decided to be a step that was not only unneccesarily complicated but perhaps unnecessary overall."],"metadata":{"id":"hMC4Aux4jfHn"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","\n","# class to support creating spectrograms within tf\n","class EEGSpectrogramLayer(Layer):\n","    def __init__(self, fs=500, nperseg=100, noverlap=0, **kwargs):\n","        super(EEGSpectrogramLayer, self).__init__(**kwargs)\n","        self.fs = fs\n","        self.nperseg = nperseg\n","        self.noverlap = noverlap\n","\n","    def call(self, inputs):\n","        def _compute_spec(x_np):\n","            specs = []\n","            for i in range(x_np.shape[0]):  # loop over neurons\n","                f, t, Sxx = scipy.signal.spectrogram(\n","                    x_np[i],\n","                    fs=self.fs,\n","                    nperseg=self.nperseg,\n","                    noverlap=self.noverlap,\n","                )\n","                specs.append(Sxx)\n","            return np.stack(specs, axis=-1)  # Shape: (freq, time, neurons)\n","\n","        specs = tf.numpy_function(_compute_spec, [inputs], tf.float32)\n","        specs.set_shape([None, None, inputs.shape[0]])  # freq, time, channels\n","        return specs\n"],"metadata":{"id":"LQZsfOwpivGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the very first training performed on the raw data using five classes. This archive is particularly useful in that it demonstrates how much relatively easier it is to achieve high accuracy on only five classes, as well the early suggestions that our models could get away with relatively few parameters given the data complexity."],"metadata":{"id":"eU_glyRolHcS"}},{"cell_type":"code","source":["# 1D CNN architecture that trains directly on data loaded using Mike's functions\n","\n","input_shape = (508, total_trials)\n","output_shape = 5\n","\n","model = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Conv1D(128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n","        layers.Conv1D(128, kernel_size=5, activation=\"relu\", padding=\"same\"),\n","        layers.Conv1D(128, kernel_size=7, activation=\"relu\", padding=\"same\"),\n","        layers.Dropout(0.75),\n","        layers.MaxPooling1D(pool_size=3),\n","        layers.Conv1D(256, kernel_size=3, activation=\"relu\", padding=\"same\"),\n","        layers.Conv1D(256, kernel_size=5, activation=\"relu\", padding=\"same\"),\n","        layers.Dropout(0.75),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Conv1D(512, kernel_size=3, activation=\"relu\", padding=\"same\"),\n","        layers.Dropout(0.75),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Flatten(),\n","        layers.Dense(512, activation=\"relu\"),\n","        layers.Dropout(0.75),\n","        layers.Dense(output_shape, activation=\"softmax\"),\n","    ]\n",")\n","\n","model.summary()"],"metadata":{"id":"jcS0QC2RlGuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Best accuracies for differnet parameter numbers\n","##### Note: 5 and 10 million use 2D\n","best_5_million = 0.863\n","\n","best_10_million = 0.867\n","\n","best_12_million = 0.82\n","\n","best_3_million = 0.852\n","\n","best_1_1_million = 0.84\n","\n","best_780K = 0.848\n","\n","best_600K = 0.852\n","\n","best_550K = 0.84\n","\n","best_376K = 0.848\n","\n","best_285K = 0.82\n","#### Overfitting seems to be less of a problem around here\n","best_200K = 0.84\n","\n","best_98K = 0.796\n","\n","best_76K = 0.793\n","\n","best_54K = 0.801\n","\n","best_37K = 0.77\n","\n","best_26K = 0.84\n","\n","best_15K =  0.781\n","\n","best_12K = 0.801\n","\n","best_4K = 0.738"],"metadata":{"id":"Ik9WltALlbhj"}},{"cell_type":"markdown","source":["This is a small note on early testing results. Important is its suggestion that LSTMs are useful in learning the data.\n","\n","'''\n","Saving best spectorgram results here\n","separating labels for left and right top accuracy is 56.8%\n","10 labels and LSTM -best is 67.9%\n","generally got about 80% with different models using all data and 5 classes\n","'''"],"metadata":{"id":"mbNojVKqmSAd"}},{"cell_type":"markdown","source":["Below is an early and poor attempt at making spectrograms"],"metadata":{"id":"5jepom1HmsZb"}},{"cell_type":"code","source":["#  New FFT functions\n","from scipy.signal import spectrogram\n","print(epoched_total.shape)\n","X_multichannel = epoched_total.reshape(1575, 508, 256, 1) #.reshape(130048, 175, 1) Ensure the shape matches (508, 35, 1)\n","\n","X = epoched_total.transpose(2, 3, 0, 1).reshape(-1, 256, 508)  # (175, 256, 508)\n","y_multichannel = np.tile(np.arange(5), 315)  # Labels: [0, 1, 2, 3, 4, 0, 1, 2, ...]\n","\n","print(\"New X shape:\", X_multichannel.shape)  # (175, 256, 508)\n","print(\"New y shape:\", y_multichannel.shape)  # (175,)\n","\n","def compute_eeg_fft_spectrogram(eeg_data, fs=250, window_size=25, step_size=10, fmax=50):\n","  num_channels, num_samples = eeg_data.shape\n","  freqs = np.fft.rfftfreq(window_size, d=1/fs)\n","  max_bin = np.argmax(freqs > fmax) if np.any(freqs > fmax) else len(freqs)\n","  num_windows = (num_samples - window_size) // step_size + 1\n","  spec = np.zeros((num_channels, max_bin, num_windows))\n","\n","  for ch in range(num_channels):\n","    for i in range(num_windows):\n","      start = i * step_size\n","      end = start + window_size\n","      segment = eeg_data[ch, start:end]\n","      fft_vals = np.abs(rfft(segment))[:max_bin]\n","      spec[ch, :, i] = fft_vals\n","\n","  return spec, freqs[:max_bin]\n","\n","fmax = 50\n","\n","ffts_images = []\n","for i in range(X.shape[0]):# Get a longer EEG window (1 second)\n","  real_epoched = X[i, :, :, 0].T  # (channels, samples)\n","\n","  fft_img = compute_eeg_fft_spectrogram(real_epoched, fs=250, window_size=25, step_size=10, fmax=50)[0]\n","  ffts_images.append(fft_img)\n","\n","plt.figure(figsize=(10, 6))\n","plt.imshow( np.log1p(spectrogram[0]), origin='lower', aspect='auto',\n","    cmap='viridis', extent=[0, spectrogram.shape[2], 0, spectrogram.shape[1]])\n","plt.title(f\"Spectrogram - Channel {ch_idx}\")\n","plt.xlabel(\"Time Window\")\n","plt.ylabel(\"Frequency (Hz)\")\n","plt.colorbar(label=\"Log Magnitude\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_rOTt1Cvlb39","executionInfo":{"status":"ok","timestamp":1750175672717,"user_tz":-120,"elapsed":8,"user":{"displayName":"Joshua Tapper","userId":"12596847093048438380"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["This code was used to isolate the smoothing process and asses its impact on accuracy."],"metadata":{"id":"VtqApKIGnjwz"}},{"cell_type":"code","source":["# Code for smoothing\n","EEG_left_smoothed = np.empty_like(epoched_left)\n","for trial_idx in range(epoched_left.shape[2]):\n","    for class_idx in range(epoched_left.shape[3]):\n","        # Extract the time x channel data for the current trial and class\n","        eeg_trial_class = epoched_left[:, :, trial_idx, class_idx].T # Transpose to get (time, channels)\n","        # Apply smoothing and assign to the new array\n","        EEG_left_smoothed[:, :, trial_idx, class_idx] = smooth_outliers(eeg_trial_class).T # Transpose back to original shape\n","\n","del epoched_left\n","\n","EEG_right_smoothed = np.empty_like(epoched_right)\n","for trial_idx in range(epoched_right.shape[2]):\n","    for class_idx in range(epoched_right.shape[3]):\n","        # Extract the time x channel data for the current trial and class\n","        eeg_trial_class = epoched_right[:, :, trial_idx, class_idx].T # Transpose to get (time, channels)\n","        # Apply smoothing and assign to the new array\n","        EEG_right_smoothed[:, :, trial_idx, class_idx] = smooth_outliers(eeg_trial_class).T # Transpose back to original shape\n","\n","del epoched_right"],"metadata":{"id":"sPW0uWNAnpPD"},"execution_count":null,"outputs":[]}]}